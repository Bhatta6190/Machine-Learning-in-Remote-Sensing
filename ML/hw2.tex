\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage{hyperref}

\title{Applications of Machine Learning in Remote Sensing\\ Homework 2}
\date{\vspace{-5ex}}
\author{John Smith -- \texttt{johnsmith@rit.edu}}

\begin{document}
  \maketitle
\begin{center}
\texttt{https://github.com/johnsmith/repo.git}\\
\end{center}

\begin{itemize}

\item Compile your solution to the problems and submit your assignment as a PDF file.

\item Paste your results and the corresponding code directly under the problem definition.

\item Ensure all text/images are legible and organized.

\item Your submitted code will be double-checked with the GitHub repository. Ensure that the code in your submission aligns with what is in the repository.

\item Alongside your results, provide an explanation of how your code works, especially for non-trivial parts of the implementation. This helps demonstrate your understanding of the problem.

\item Please comment your code. 

\item Ensure that your code can reproduce the submitted results.

\item Include an \textit{entry point check} so I could run your code easily. This also ensures that your code is not ran accidentally if you were to call these functions from somewhere else. 

\begin{verbatim}
    if __name__ == "__main__":
        csv_file = "path/to/data.csv"
        function(csv_file)
\end{verbatim}


\end{itemize}


\newpage
\noindent Create a directory in your repository and name it \texttt{ml}, if you already do not have. The workflows and the scripts created in this homework would go under \texttt{ml}.\\


\noindent\textbf{Do not leave the file in your github repository for your homework as the data file is large.}

\section*{Problem 1: Hyperspectral Data}

Last week, you worked with Sentinel-2 multispectral data. This week, you will analyze hyperspectral data collected by the RIT MX1 drone over the RIT Tait Reserve area. The dataset is provided as \texttt{taitlabsphere} and \texttt{taitlabsphere.hdr}.\\

\noindent This is an ENVI headered file, which includes a corresponding metadata file (.hdr). The header file describes various aspects of the dataset, such as the coordinate system, number of lines and columns, number of spectral bands, and more. Read through the .hdr file to familiarize yourself with the metadata structure. You can also find the wavelengths for each spectral band within the header file. \\


\noindent\textbf{(1.a)(10 points)} Load the data using the Spectral Python (SPy) and envi module, select and plot one band from the blue, green, and red regions of the spectrum. Additionally, create a pseudocolor image using bands from the green, red, and near-infrared (NIR) regions.\\

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(1.b)(10 points)} Compute and display the correlation matrix as an image (from the previous homework). Analyze the correlation between spectral bands—what patterns do you observe? How are the bands related to each other?  \\

\noindent\textbf{Solution}:\\
Put your solution here\\

\section*{Problem 2: Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) is often explained using \textbf{eigendecomposition} of the covariance matrix. However, due to \textbf{finite-precision arithmetic}, this approach can be numerically unstable on a computer. Instead, in practice, Singular Value Decomposition (SVD) is used to compute PCA more robustly.\\

\noindent\textbf{(2.a)(10 points)} Write a function that takes an array of size \textbf{\(m \times d\)}, (where \(m\) is the number of samples and \(d\) is the number of features). Your function should return 1) all principal components as an array, 2) eigenvalues, and 3) standardized data.\\

\noindent\textbf{Note:} 
\begin{itemize}
    \item In this case, mean-centering your data prior to passing to SVD should be enough, as the data is of a certain bit and in physical units. But keep in mind, for features across different units and scales, you will need to perform standardization (mean-centering + dividing by standard deviation) as each features can have different range of values and scales.
    \item Mean-centering should be done on each feature separately.
    \item Do not use a libary for PCA calculation, you may use a library for SVD calculation.
    \item Make sure the eigenvalues and corr. eigenvectors are sorted
    \item Provide sufficient explanation in your code
\end{itemize}

\begin{verbatim}
    pcs, eigenvalues, mean_arr = principal_component_analysis(array)
\end{verbatim}

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(2.b)(10 points)} Using PCA as a dimensionality reduction technique, apply PCA to the provided hyperspectral dataset. Extract the first 10 principal components, use them to transform your data into the lower dimension, reshape the the observation dimension to match the image dimensions, and plot them. Describe your observations—what patterns or features do you notice? Discuss what each principal component represents in the context of the hyperspectral data.\\

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(2.c)(10 points)} On the same hyperspectral dataset, plot the mean reconstruction error using the L2 distance (between the mean-centered and reconstructed data) as a function of the number of principal components. Given the large size of the image dataset, compute and visualize the error for a selected number of principal components: 1, 10, 50, 100, and  \(d \)  (the total number of features).\\

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(2.d) (10 points)} As you may have observed, principal components (PCs) with lower eigenvalues primarily capture noise rather than meaningful variability. This property can be leveraged to reduce noise in high-dimensional data. 

On the hyperspectral dataset, using the eigenvalues and their corresponding PCs, compute the explained variance ratio for each PC (eigenvalue of a PC divided by the sum of all eigenvalues). Retain the PCs that collectively explain 99\% of the total variance, and set the remaining PCs to zero. Then, apply the inverse principal component transformation to reconstruct the data using the modified \( d \times d \) PC matrix. Note that no PCs are removed—only their contributions in the eigenvector space are manipulated. 

Select five interesting pixels from your image and compare their spectral signals before and after transformation; show this as a plot for your five pixels, make sure to name what materials you picked. Additionally, calculate and report the signal-to-noise ratio (SNR) \(\frac{\mu}{\sigma}\) before and after the transformation for the entire image. Make sure to mask out no-data pixels.\\


\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(2.BONUS)(10 points)} Implement the concept of impact plot discussed in Peter Bajorski's book provided as optional reading material in Week 2. Visualize how variability changes relative to the mean signal for the first five principal components in terms of positive and negative impacts. 

file: week2\_statistics\_for\_Imaging\_Optics\_and\_Photon.pdf, Section 7.2.3\\

\noindent\textbf{Solution}:\\
Put your solution here\\

\section*{Problem 3: K-Means Clustering}

\noindent\textbf{(3.a)(15 points)} We discussed K-Means clustering as an unsupervised clustering technique in class. Implement K-Means from scratch \texttt{without using a library}. Below is a pseudo code:

\begin{enumerate}
    \item initialize K cluster centers 
    \item Iterate until convergence or max number of iterations; new cluster centers will not change.
    \begin{enumerate}
        \item Assign each point's class to the Nearest Centroid
        \item Compute new cluster centeroids
    \end{enumerate}
\end{enumerate}

\noindent Keep in mind that K-Means calculates L2 distance across dimensions and iteratively updates cluster centers to the mean of assigned points. Since L2 distance is sensitive to scale, make sure to standardize your data before applying K-Means.  

To debug your approach, use the provided test image \texttt{jellybeans.tiff}, which is smaller and easier to analyze. Based on the colors in the image, what is an appropriate choice for \( K \)?  \\

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(3.b)(15 points)} In this exercise, we will use the Sentinel-2 data from Assignment 1. Provided Sentinel-2 has 12 spectral bands, but L2 distance in high-dimensional spaces (many features) can become less meaningful. This raises the question of which features should be used for unsupervised clustering?  

One approach is to select specific bands relevant to the clusters of interest based on our application. Another approach is to use PCA as a feature extraction technique.  

\begin{itemize}
\item Apply PCA to the Sentinel-2 data using your PCA function.  
\item Extract the first 3, 4, 5, and 6 principal components and transform the data into these lower-dimensional representations.  
\item Apply your K-Means clustering algorithm to the transformed data.\\
\end{itemize}

\noindent\textbf{Solution}:\\
Put your solution here\\

\noindent\textbf{(3.c)(10 points)} Now that you have used PCA for feature extraction and K-Means for clustering, we will apply the same workflow to hyperspectral data from Problem 2. In this case, we have hundreds of bands, making feature selection non-trivial. Additionally, since this dataset is much larger in terms of number of samples, K-Means will be computationally expensive due to repeated distance calculations. To improve efficiency, use MiniBatch K-Means from \texttt{sklearn} library, which speeds up clustering for dataset of large sizes.  

For this problem:
\begin{itemize}
    \item Extract a \(250 \times 250\) patch from the hyperspectral dataset that includes calibration panels, roads, and vegetation (regions with variability).  
    \item Perform dimensionality reduction using PCA, transforming the data to 2, 5, 10, 50, and 100 features.  
    \item Apply K-Means clustering to both the lower-dimensional data and the original dataset (all bands).  
    \item Report and compare your results. How does performance change with different numbers of features?  
\end{itemize}\\

\noindent\textbf{Solution}:\\
Put your solution here\\


\end{document}