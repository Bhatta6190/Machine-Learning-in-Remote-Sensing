# ## Data reduction using PCA for classification

# ## Function to compute Principal components from given hyperspectral data:

# def principal_component_analysis(array):

#     """
#     Here, array is the flattened image i.e if the original image size is m * n * k, `array` would be of size : (m*n = samples, k = bands)
#     """

#     # Mean centering the array across all bands 
#     array = (array - np.mean(array, axis=0))

#     # using SVD to find U, S and V (eigen vectors) matrices.
#     U, S, VT = np.linalg.svd(array, full_matrices=False)

#     # Principal components
#     pcs = VT.T

#     # Eigen values
#     eigen_values = (S**2)/(array.shape[0] - 1)
    
#     return pcs, eigen_values, array

# pcs, eigen_values, mean_centered_data = principal_component_analysis(valid_hsi_data)

# ## Analysis of PC components: How many bands can explain almost all the information in original dataset?

# # Explained variance ratio calculation for all eigen vectors
# ev_ratio = eigen_values / np.sum(eigen_values)
# cum_variance = np.cumsum(ev_ratio)

# # Extract the PCs that explaine 99% variance
# num_retained_pcs = np.argmax(cum_variance >= 0.99) + 1

# ## Plotting the cumulative variance explained

# plt.figure(figsize=(10, 5))
# plt.plot([i+1 for i in range(len(cum_variance))], cum_variance, color='g', linestyle='dashed')
# plt.axvline(num_retained_pcs, color='r', linestyle='dashed', label=f'{num_retained_pcs} PC bands' )
# plt.xlabel("Number of PCs")
# plt.ylabel("Cumulative Variance Explained")
# plt.title(f"Cumulative Variance vs. Number of PCs\n({num_retained_pcs} PC bands explain 99% variance in original data)")
# plt.minorticks_on()
# plt.legend()
# plt.grid(which='both')
# plt.show()

# ## Reducing the dataset to 3 PC bands

# num_components = num_retained_pcs
# first_3pcs = pcs[:, :num_components]
# reduced_hsi_data = np.dot(mean_centered_data, first_3pcs)
# reduced_hsi_data.shape

# ## Plot the PC reduced Data

# pc_space_data = np.full((m * n, num_components), 0)  # Initialize zero array
# pc_space_data[valid_mask] = reduced_hsi_data  # Fill with reconstructed values
# pc_image = pc_space_data.reshape(m, n, num_components)  # Reshape into (m, n, d)

# fig, axes = plt.subplots(1, 3, figsize=(12, 6))
# axes[0].imshow(pc_image[:,:,0], cmap='gray')  
# axes[0].set_title('PC band 1')
# axes[1].imshow(pc_image[:,:,1], cmap='gray')  
# axes[1].set_title(f'PC band 2')
# axes[2].imshow(pc_image[:,:,2], cmap='gray')  
# axes[2].set_title(f'PC band 3')
# plt.show()

# ## Using Random undersampling for class balancing

# from sklearn.utils import resample
# import numpy as np

# target_size = 947 ## Setting it to Smallest class size

# # Assign Resample list
# undersampled_data = []
# undersampled_labels = []

# unique_classes = np.unique(valid_gt_pixels)

# for cls in unique_classes:
#     class_mask = valid_gt_pixels == cls
#     class_data = reduced_hsi_data[class_mask]
#     class_labels = valid_gt_pixels[class_mask]

#     # Random undersampling to target size
#     class_data_resampled, class_labels_resampled = resample(
#         class_data, class_labels,
#         replace=False,  # No replacement (undersampling)
#         n_samples=target_size,
#         random_state=42
#     )

#     undersampled_data.append(class_data_resampled)
#     undersampled_labels.append(class_labels_resampled)

# # Convert to numpy arrays
# undersampled_data = np.vstack(undersampled_data)
# undersampled_labels = np.hstack(undersampled_labels)

# print(f"New dataset shape: {undersampled_data.shape}, Labels shape: {undersampled_labels.shape}")

# ## Seperate into classes of interest

# veg_classes = [2, 4]  # Trees & Meadows

# # Assign binary labels: 1 for vegetation, 0 for non-vegetation
# binary_labels = np.isin(undersampled_labels, veg_classes).astype(int)

# print(f"Class distribution after binarization: {np.bincount(binary_labels)}")

# ## 90-10 split

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(
#     undersampled_data, binary_labels, 
#     test_size=0.2, random_state=42
# )

# print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")

# ## Training a logistic regression model

# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
# import matplotlib.pyplot as plt

# # Train logistic regression model
# log_reg = LogisticRegression(random_state=42)
# log_reg.fit(X_train, y_train)

# # Make predictions
# y_train_pred = log_reg.predict(X_train)
# y_test_pred = log_reg.predict(X_test)

# # Get prediction probabilities for ROC curve
# y_test_prob = log_reg.predict_proba(X_test)[:, 1]  # Probabilities for class 1 (Vegetation)

# ## Evaluation metrics

# # Compute metrics
# print("Training Performance:")
# print(classification_report(y_train, y_train_pred))

# print("Testing Performance:")
# print(classification_report(y_test, y_test_pred))

# ## ROC curve

# # Compute ROC curve
# fpr, tpr, _ = roc_curve(y_test, y_test_prob)
# roc_auc = auc(fpr, tpr)

# # Plot ROC curve
# plt.figure(figsize=(6, 6))
# plt.plot(fpr, tpr, color='red', lw=1, label=f'AUC = {roc_auc:.2f}', linestyle='--')
# plt.plot([0, 1], [0, 1], color='g', linestyle='--')
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('ROC Curve for Logistic Regression')
# plt.legend(loc='lower right')
# plt.show()